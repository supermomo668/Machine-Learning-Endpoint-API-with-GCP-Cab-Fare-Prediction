trainingInput:
  scaleTier: CUSTOM
  masterType: standard # You may set it to "large_model" if you want to increase the memory
  workerType: standard
  workerCount: 2 # You may increase the number of workers to run multiple trials in parallel
  # Documents that you should read on hypertuning configuration:
  # https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec
  hyperparameters:
    hyperparameterMetricTag: nyc_fare
    goal: MINIMIZE # i.e., the goal is to minimize the RMSE
    # You can adjust maxTrials to balance the cost and the rounds of tuning
    maxTrials: 15
    maxParallelTrials: 3
    enableTrialEarlyStopping: TRUE
    params:
    - parameterName: max_depth
      # This hyperparamter can be used to control over-fitting,
      # because a larger depth will make the model to learn relations
      # in a more specific way for particular training samples
      #
      # This entry matches the following code in ai_platform_trainer/train.py
      #    parser.add_argument(
      #        '--max_depth',
      #        default=6,
      #        type=int
      #    )
      type: INTEGER
      minValue: 10
      maxValue: 12
    # TODO: Add at least 3 more parameters to be tuned by HyperTune
    # You need to update both config.yaml and ai_platform_trainer/train.py
    - parameterName: reg_lambda
      type: DOUBLE
      minValue: 0.2
      maxValue: 0.5
      scaleType: UNIT_LINEAR_SCALE
      
    - parameterName: n_estimators
      type: INTEGER
      minValue: 2
      maxValue: 4
      
    - parameterName: learning_rate
      type: DOUBLE
      minValue: 0.2
      maxValue: 0.4
      
       